\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{JK}
\citation{matt2024}
\citation{kashyap-2023}
\citation{buchholz-2024}
\citation{verwimp2024continuallearningapplicationsroad}
\citation{verwimp2024continuallearningapplicationsroad}
\citation{ibm2025}
\citation{Sudmann2020}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\citation{LW,verwimp2024continuallearningapplicationsroad,bidaki2025}
\citation{Du_2019}
\citation{Du_2019}
\citation{Fahrmeir_2022}
\citation{Fahrmeir_2022}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks (NN)}{2}{section.2}\protected@file@percent }
\newlabel{nn}{{2}{2}{Neural Networks (NN)}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A one-neuron perceptron (left) and a one-layer perceptron with 3 neurons (right).}}{3}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:P}{{1}{3}{A one-neuron perceptron (left) and a one-layer perceptron with 3 neurons (right)}{figure.caption.3}{}}
\citation{LW}
\citation{LW}
\citation{bidaki2025}
\citation{bidaki2025}
\citation{LW}
\citation{LW}
\citation{bidaki2025,LW}
\@writefile{toc}{\contentsline {section}{\numberline {3}Framework}{4}{section.3}\protected@file@percent }
\newlabel{framework}{{3}{4}{Framework}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scenarios}{4}{subsection.3.1}\protected@file@percent }
\newlabel{scenarios}{{3.1}{4}{Scenarios}{subsection.3.1}{}}
\citation{vandeVen2022}
\citation{vandeVen2022}
\citation{aljundi2019tfcl}
\citation{bidaki2025,LW}
\citation{bidaki2025}
\citation{vandeVen2022}
\citation{bidaki2025,LW}
\citation{bidaki2025,LW}
\citation{bidaki2025,LW}
\citation{FRENCH1999128,Mcclelland1995,MCCLOSKEY1989109,Ratcliff1990ConnectionistMO}
\citation{evron2022}
\citation{evron2022}
\citation{Shalev-Shwartz}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stability-Plasticity Trade-off}{6}{subsection.3.2}\protected@file@percent }
\newlabel{cf}{{3.2}{6}{Stability-Plasticity Trade-off}{subsection.3.2}{}}
\citation{chaudhry2019,rebuffi2017icarlincrementalclassifierrepresentation,aljundi2019gradientbasedsampleselection}
\citation{lopezpaz2022gradientepisodicmemorycontinual,javed2019metalearningrepresentationscontinuallearning,mirzadeh2020understandingroletrainingregimes}
\citation{mallya2018piggybackadaptingsinglenetwork,ebrahimi2020adversarialcontinuallearning,fernando2017pathnetevolutionchannelsgradient}
\citation{lopezpaz2022gradientepisodicmemorycontinual}
\citation{lopezpaz2022gradientepisodicmemorycontinual}
\citation{lopezpaz2022gradientepisodicmemorycontinual}
\citation{lopezpaz2022gradientepisodicmemorycontinual}
\citation{díazrodríguez2018dontforgetforgettingnew}
\citation{díazrodríguez2018dontforgetforgettingnew}
\@writefile{toc}{\contentsline {section}{\numberline {4}Metrics}{7}{section.4}\protected@file@percent }
\newlabel{metrics}{{4}{7}{Metrics}{section.4}{}}
\citation{lopezpaz2022gradientepisodicmemorycontinual}
\citation{LW}
\citation{mirzadeh2020understandingroletrainingregimes}
\citation{mirzadeh2020understandingroletrainingregimes}
\citation{mirzadeh2020understandingroletrainingregimes}
\citation{mirzadeh2020understandingroletrainingregimes}
\citation{mirzadeh2020understandingroletrainingregimes}
\newlabel{2TA}{{14}{9}{Metrics}{equation.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Depending on the curvature of $L_1$ and $L_2$, different $\Delta w$ result in different forgetting rates.}}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:02}{{2}{9}{Depending on the curvature of $L_1$ and $L_2$, different $\Delta w$ result in different forgetting rates}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Regularization}{9}{section.5}\protected@file@percent }
\newlabel{reg0}{{5}{9}{Regularization}{section.5}{}}
\citation{evron2022,li2024fixeddesignanalysisregularizationbased}
\citation{evron2022,li2024fixeddesignanalysisregularizationbased,zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{li2024fixeddesignanalysisregularizationbased}
\citation{li2024fixeddesignanalysisregularizationbased}
\citation{li2024fixeddesignanalysisregularizationbased,zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{li2024fixeddesignanalysisregularizationbased}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Regularization of Parameter Space}{11}{subsection.5.1}\protected@file@percent }
\newlabel{reg01}{{5.1}{11}{Regularization of Parameter Space}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Quadratic Penalties}{11}{subsubsection.5.1.1}\protected@file@percent }
\newlabel{reg011}{{5.1.1}{11}{Quadratic Penalties}{subsubsection.5.1.1}{}}
\newlabel{ridge}{{17}{11}{Quadratic Penalties}{equation.5.17}{}}
\newlabel{ridgeMat}{{19}{11}{Quadratic Penalties}{equation.5.19}{}}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual}
\citation{JK}
\citation{JK}
\citation{JK}
\citation{zhao2024statisticaltheoryregularizationbasedcontinual,zenke2017continuallearningsynapticintelligence,Husz_r_2018,li2024fixeddesignanalysisregularizationbased,titsias2020functionalregularisationcontinuallearning,yin2021optimizationgeneralizationregularizationbasedcontinual,loo2020generalizedvariationalcontinuallearning,benzing2021unifyingregularisationmethodscontinual}
\citation{JK}
\citation{JK}
\newlabel{oracle}{{20}{12}{Quadratic Penalties}{equation.5.20}{}}
\citation{zenke2017continuallearningsynapticintelligence}
\citation{zenke2017continuallearningsynapticintelligence}
\newlabel{ewcBayes}{{22}{13}{Quadratic Penalties}{equation.5.22}{}}
\newlabel{EWC}{{23}{13}{Quadratic Penalties}{equation.5.23}{}}
\citation{aljundi2018memoryawaresynapseslearning}
\citation{aljundi2018memoryawaresynapseslearning}
\citation{aljundi2018memoryawaresynapseslearning}
\citation{benzing2021unifyingregularisationmethodscontinual}
\citation{benzing2021unifyingregularisationmethodscontinual}
\citation{yin2021optimizationgeneralizationregularizationbasedcontinual}
\citation{yin2021optimizationgeneralizationregularizationbasedcontinual}
\citation{liu2018rotatenetworksbetterweight}
\citation{liu2018rotatenetworksbetterweight}
\newlabel{SI}{{24}{14}{Quadratic Penalties}{equation.5.24}{}}
\newlabel{MAS}{{25}{14}{Quadratic Penalties}{equation.5.25}{}}
\newlabel{l2pen}{{26}{14}{Quadratic Penalties}{equation.5.26}{}}
\citation{yin2021optimizationgeneralizationregularizationbasedcontinual}
\citation{jung2021continuallearningnodeimportancebased}
\citation{yoon2018lifelonglearningdynamicallyexpandable}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Sparse Penalties}{15}{subsubsection.5.1.2}\protected@file@percent }
\newlabel{reg012}{{5.1.2}{15}{Sparse Penalties}{subsubsection.5.1.2}{}}
\newlabel{agscl}{{28}{15}{Sparse Penalties}{equation.5.28}{}}
\citation{yoon2018lifelonglearningdynamicallyexpandable}
\citation{Gou_2021}
\citation{li2017learningforgetting}
\citation{li2017learningforgetting}
\citation{hinton2015distillingknowledgeneuralnetwork}
\citation{Wang_Liu_Duan_Tao_2022}
\citation{Wang_Liu_Duan_Tao_2022}
\citation{welling_2009}
\citation{welling_2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Regularization of Function Space}{17}{subsection.5.2}\protected@file@percent }
\newlabel{reg02}{{5.2}{17}{Regularization of Function Space}{subsection.5.2}{}}
\newlabel{LwF}{{29}{17}{Regularization of Function Space}{equation.5.29}{}}
\citation{Wang_Liu_Duan_Tao_2022}
\citation{yin2021optimizationgeneralizationregularizationbasedcontinual}
\citation{yin2021optimizationgeneralizationregularizationbasedcontinual}
\citation{titsias2020functionalregularisationcontinuallearning}
\citation{titsias2020functionalregularisationcontinuallearning}
\citation{Ludkovski2025}
\citation{titsias2020functionalregularisationcontinuallearning}
\citation{Ludkovski2025}
\newlabel{dri}{{30}{18}{Regularization of Function Space}{equation.5.30}{}}
\newlabel{bFrcl}{{31}{18}{Regularization of Function Space}{equation.5.31}{}}
\citation{Blei_2017}
\newlabel{elbo}{{33}{19}{Regularization of Function Space}{equation.5.33}{}}
\citation{zenke2017continuallearningsynapticintelligence,aljundi2018memoryawaresynapseslearning,yoon2018lifelonglearningdynamicallyexpandable,jung2021continuallearningnodeimportancebased,titsias2020functionalregularisationcontinuallearning}
\citation{zenke2017continuallearningsynapticintelligence,yoon2018lifelonglearningdynamicallyexpandable,jung2021continuallearningnodeimportancebased,Wang_Liu_Duan_Tao_2022}
\newlabel{penElbo}{{35}{20}{Regularization of Function Space}{equation.5.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Notes on Evaluation}{20}{section.6}\protected@file@percent }
\newlabel{eval}{{6}{20}{Notes on Evaluation}{section.6}{}}
\citation{LW}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{21}{section.7}\protected@file@percent }
\newlabel{conclusion}{{7}{21}{Conclusion}{section.7}{}}
\citation{JK}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{IV}{appendix.A}\protected@file@percent }
\newlabel{app}{{A}{IV}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Expansion of eq. 2 in \cite  {JK} for \(t\) samples \eqref  {ewcBayes}}{IV}{subsection.A.1}\protected@file@percent }
\newlabel{ewcB}{{A.1}{IV}{Expansion of eq. 2 in \cite {JK} for \(t\) samples \eqref {ewcBayes}}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}CRR is unbiased}{IV}{subsection.A.2}\protected@file@percent }
\newlabel{crr}{{A.2}{IV}{CRR is unbiased}{subsection.A.2}{}}
\bibstyle{abbrvnat}
\bibdata{bibliography}
\bibcite{aljundi2018memoryawaresynapseslearning}{{1}{2018}{{Aljundi et~al.}}{{Aljundi, Babiloni, Elhoseiny, Rohrbach, and Tuytelaars}}}
\bibcite{aljundi2019tfcl}{{2}{2019{}}{{Aljundi et~al.}}{{Aljundi, Kelchtermans, and Tuytelaars}}}
\bibcite{aljundi2019gradientbasedsampleselection}{{3}{2019{}}{{Aljundi et~al.}}{{Aljundi, Lin, Goujaud, and Bengio}}}
\bibcite{benzing2021unifyingregularisationmethodscontinual}{{4}{2021}{{Benzing}}{{}}}
\bibcite{bidaki2025}{{5}{2025}{{Bidaki et~al.}}{{Bidaki, Mohammadkhah, Rezaee, Hassani, Eskandari, Salahi, and Ghassemi}}}
\bibcite{Blei_2017}{{6}{2017}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{chaudhry2019}{{7}{2019}{{Chaudhry et~al.}}{{Chaudhry, Rohrbach, Elhoseiny, Ajanthan, Dokania, Torr, and Ranzato}}}
\bibcite{Du_2019}{{8}{2019}{{Du and Swamy}}{{}}}
\bibcite{díazrodríguez2018dontforgetforgettingnew}{{9}{2018}{{Díaz-Rodríguez et~al.}}{{Díaz-Rodríguez, Lomonaco, Filliat, and Maltoni}}}
\bibcite{ebrahimi2020adversarialcontinuallearning}{{10}{2020}{{Ebrahimi et~al.}}{{Ebrahimi, Meier, Calandra, Darrell, and Rohrbach}}}
\bibcite{evron2022}{{11}{2022}{{Evron et~al.}}{{Evron, Moroshko, Ward, Srebro, and Soudry}}}
\bibcite{Fahrmeir_2022}{{12}{2022}{{Fahrmeir et~al.}}{{Fahrmeir, Kneib, Lang, and Marx}}}
\bibcite{fernando2017pathnetevolutionchannelsgradient}{{13}{2017}{{Fernando et~al.}}{{Fernando, Banarse, Blundell, Zwols, Ha, Rusu, Pritzel, and Wierstra}}}
\bibcite{FRENCH1999128}{{14}{1999}{{French}}{{}}}
\bibcite{kashyap-2023}{{15}{}{{given i=D}}{{}}}
\bibcite{buchholz-2024}{{16}{}{{given i=K}}{{}}}
\bibcite{Gou_2021}{{17}{2021}{{Gou et~al.}}{{Gou, Yu, Maybank, and Tao}}}
\bibcite{hassija2024}{{18}{2024}{{Hassija et~al.}}{{Hassija, Chamola, and et~al.}}}
\bibcite{hinton2015distillingknowledgeneuralnetwork}{{19}{2015}{{Hinton et~al.}}{{Hinton, Vinyals, and Dean}}}
\bibcite{Husz_r_2018}{{20}{2018}{{Huszár}}{{}}}
\bibcite{ibm2025}{{21}{I.}{{}}{{()}}}
\bibcite{javed2019metalearningrepresentationscontinuallearning}{{22}{2019}{{Javed and White}}{{}}}
\bibcite{jung2021continuallearningnodeimportancebased}{{23}{2021}{{Jung et~al.}}{{Jung, Ahn, Cha, and Moon}}}
\bibcite{JK}{{24}{2017}{{Kirkpatrick et~al.}}{{Kirkpatrick, Pascanu, Rabinowitza, Veness, Guillaume~Desjardins, Milan, Quan, Ramalho, Agnieszk Grabska-Barwinska, Clopath, Kumaran, and Hadsell}}}
\bibcite{li2024fixeddesignanalysisregularizationbased}{{25}{2024}{{Li et~al.}}{{Li, Wu, and Braverman}}}
\bibcite{li2017learningforgetting}{{26}{2017}{{Li and Hoiem}}{{}}}
\bibcite{liu2018rotatenetworksbetterweight}{{27}{2018}{{Liu et~al.}}{{Liu, Masana, Herranz, de~Weijer, Lopez, and Bagdanov}}}
\bibcite{loo2020generalizedvariationalcontinuallearning}{{28}{2020}{{Loo et~al.}}{{Loo, Swaroop, and Turner}}}
\bibcite{lopezpaz2022gradientepisodicmemorycontinual}{{29}{2022}{{Lopez-Paz and Ranzato}}{{}}}
\bibcite{Ludkovski2025}{{30}{2025}{{Ludkovski and Risk}}{{}}}
\bibcite{mallya2018piggybackadaptingsinglenetwork}{{31}{2018}{{Mallya et~al.}}{{Mallya, Davis, and Lazebnik}}}
\bibcite{matt2024}{{32}{2024}{{Matt and Matt}}{{}}}
\bibcite{Mcclelland1995}{{33}{1995}{{Mcclelland et~al.}}{{Mcclelland, Mcnaughton, and O'Reilly}}}
\bibcite{MCCLOSKEY1989109}{{34}{1989}{{McCloskey and Cohen}}{{}}}
\bibcite{mirzadeh2020understandingroletrainingregimes}{{35}{2020}{{Mirzadeh et~al.}}{{Mirzadeh, Farajtabar, Pascanu, and Ghasemzadeh}}}
\bibcite{Ratcliff1990ConnectionistMO}{{36}{1990}{{Ratcliff}}{{}}}
\bibcite{rebuffi2017icarlincrementalclassifierrepresentation}{{37}{2017}{{Rebuffi et~al.}}{{Rebuffi, Kolesnikov, Sperl, and Lampert}}}
\bibcite{Shalev-Shwartz}{{38}{2014}{{Shalev-Shwartz and Ben-David}}{{}}}
\bibcite{Sudmann2020}{{39}{2020}{{Sudmann}}{{}}}
\bibcite{titsias2020functionalregularisationcontinuallearning}{{40}{2020}{{Titsias et~al.}}{{Titsias, Schwarz, de~G.~Matthews, Pascanu, and Teh}}}
\bibcite{vandeVen2022}{{41}{2022}{{van~de Ven et~al.}}{{van~de Ven, Tuytelaars, and Tolias}}}
\bibcite{verwimp2024continuallearningapplicationsroad}{{42}{2024}{{Verwimp et~al.}}{{Verwimp, Aljundi, Ben-David, Bethge, Cossu, Gepperth, Hayes, Hüllermeier, Kanan, Kudithipudi, Lampert, Mundt, Pascanu, Popescu, Tolias, van~de Weijer, Liu, Lomonaco, Tuytelaars, and van~de Ven}}}
\bibcite{LW}{{43}{2024}{{Wang et~al.}}{{Wang, Zhang, Su, Zhu, Fellow, and IEEE}}}
\bibcite{Wang_Liu_Duan_Tao_2022}{{44}{2022}{{Wang et~al.}}{{Wang, Liu, Duan, and Tao}}}
\bibcite{welling_2009}{{45}{2009}{{Welling}}{{}}}
\bibcite{yin2021optimizationgeneralizationregularizationbasedcontinual}{{46}{2021}{{Yin et~al.}}{{Yin, Farajtabar, Li, Levine, and Mott}}}
\bibcite{yoon2018lifelonglearningdynamicallyexpandable}{{47}{2018}{{Yoon et~al.}}{{Yoon, Yang, Lee, and Hwang}}}
\bibcite{zenke2017continuallearningsynapticintelligence}{{48}{2017}{{Zenke et~al.}}{{Zenke, Poole, and Ganguli}}}
\bibcite{zhao2024statisticaltheoryregularizationbasedcontinual}{{49}{2024}{{Zhao et~al.}}{{Zhao, Wang, Huang, and Lin}}}
\gdef \@abspage@last{34}
