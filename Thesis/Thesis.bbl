\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2018)Aljundi, Babiloni, Elhoseiny, Rohrbach, and
  Tuytelaars]{aljundi2018memoryawaresynapseslearning}
R.~Aljundi, F.~Babiloni, M.~Elhoseiny, M.~Rohrbach, and T.~Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget, 2018.
\newblock URL \url{https://arxiv.org/abs/1711.09601}.

\bibitem[Aljundi et~al.(2019{\natexlab{a}})Aljundi, Kelchtermans, and
  Tuytelaars]{aljundi2019tfcl}
R.~Aljundi, K.~Kelchtermans, and T.~Tuytelaars.
\newblock Task-free continual learning, 2019{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1812.03596}.

\bibitem[Aljundi et~al.(2019{\natexlab{b}})Aljundi, Lin, Goujaud, and
  Bengio]{aljundi2019gradientbasedsampleselection}
R.~Aljundi, M.~Lin, B.~Goujaud, and Y.~Bengio.
\newblock Gradient based sample selection for online continual learning,
  2019{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/1903.08671}.

\bibitem[Benzing(2021)]{benzing2021unifyingregularisationmethodscontinual}
F.~Benzing.
\newblock Unifying regularisation methods for continual learning, 2021.
\newblock URL \url{https://arxiv.org/abs/2006.06357}.

\bibitem[Bidaki et~al.(2025)Bidaki, Mohammadkhah, Rezaee, Hassani, Eskandari,
  Salahi, and Ghassemi]{bidaki2025}
S.~A. Bidaki, A.~Mohammadkhah, K.~Rezaee, F.~Hassani, S.~Eskandari, M.~Salahi,
  and M.~M. Ghassemi.
\newblock Online continual learning: A systematic literature review of
  approaches, challenges, and benchmarks, 2025.
\newblock URL \url{https://arxiv.org/abs/2501.04897}.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{Blei_2017}
D.~M. Blei, A.~Kucukelbir, and J.~D. McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (518):\penalty0 859–877, Apr. 2017.
\newblock ISSN 1537-274X.
\newblock \doi{10.1080/01621459.2017.1285773}.
\newblock URL \url{http://dx.doi.org/10.1080/01621459.2017.1285773}.

\bibitem[Chaudhry et~al.(2019)Chaudhry, Rohrbach, Elhoseiny, Ajanthan, Dokania,
  Torr, and Ranzato]{chaudhry2019}
A.~Chaudhry, M.~Rohrbach, M.~Elhoseiny, T.~Ajanthan, P.~K. Dokania, P.~H.~S.
  Torr, and M.~Ranzato.
\newblock On tiny episodic memories in continual learning, 2019.
\newblock URL \url{https://arxiv.org/abs/1902.10486}.

\bibitem[Du and Swamy(2019)]{Du_2019}
K.-L. Du and M.~N.~S. Swamy.
\newblock \emph{Neural Networks and Statistical Learning}.
\newblock Springer London, 2 edition, 2019.

\bibitem[Díaz-Rodríguez et~al.(2018)Díaz-Rodríguez, Lomonaco, Filliat, and
  Maltoni]{díazrodríguez2018dontforgetforgettingnew}
N.~Díaz-Rodríguez, V.~Lomonaco, D.~Filliat, and D.~Maltoni.
\newblock Don't forget, there is more than forgetting: new metrics for
  continual learning, 2018.
\newblock URL \url{https://arxiv.org/abs/1810.13166}.

\bibitem[Ebrahimi et~al.(2020)Ebrahimi, Meier, Calandra, Darrell, and
  Rohrbach]{ebrahimi2020adversarialcontinuallearning}
S.~Ebrahimi, F.~Meier, R.~Calandra, T.~Darrell, and M.~Rohrbach.
\newblock Adversarial continual learning, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.09553}.

\bibitem[Evron et~al.(2022)Evron, Moroshko, Ward, Srebro, and
  Soudry]{evron2022}
I.~Evron, E.~Moroshko, R.~Ward, N.~Srebro, and D.~Soudry.
\newblock How catastrophic can catastrophic forgetting be in linear
  regression?, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.09588}.

\bibitem[Fahrmeir et~al.(2022)Fahrmeir, Kneib, Lang, and Marx]{Fahrmeir_2022}
L.~Fahrmeir, T.~Kneib, S.~Lang, and B.~D. Marx.
\newblock \emph{Regression: Models, Methods and Applications}.
\newblock Springer Berlin, 2 edition, 2022.

\bibitem[Fernando et~al.(2017)Fernando, Banarse, Blundell, Zwols, Ha, Rusu,
  Pritzel, and Wierstra]{fernando2017pathnetevolutionchannelsgradient}
C.~Fernando, D.~Banarse, C.~Blundell, Y.~Zwols, D.~Ha, A.~A. Rusu, A.~Pritzel,
  and D.~Wierstra.
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks, 2017.
\newblock URL \url{https://arxiv.org/abs/1701.08734}.

\bibitem[French(1999)]{FRENCH1999128}
R.~M. French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in Cognitive Sciences}, 3\penalty0 (4):\penalty0
  128--135, 1999.
\newblock ISSN 1364-6613.
\newblock \doi{https://doi.org/10.1016/S1364-6613(99)01294-2}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S1364661399012942}.

\bibitem[given i=D()]{kashyap-2023}
f.~given i=D, given=Dilip.
\newblock What is chatgpt openai and how it is built: The methodology behind
  it.
\newblock URL
  \url{https://levelup.gitconnected.com/what-is-chatgpt-openai-how-it-is-built-the-technology-behind-it-ba3e8acc1e9b}.

\bibitem[given i=K()]{buchholz-2024}
f.~given i=K, given=Katharina.
\newblock The extreme cost of training ai models.
\newblock URL
  \url{https://www.forbes.com/sites/katharinabuchholz/2024/08/23/the-extreme-cost-of-training-ai-models/}.

\bibitem[Gou et~al.(2021)Gou, Yu, Maybank, and Tao]{Gou_2021}
J.~Gou, B.~Yu, S.~J. Maybank, and D.~Tao.
\newblock Knowledge distillation: A survey.
\newblock \emph{International Journal of Computer Vision}, 129\penalty0
  (6):\penalty0 1789–1819, Mar. 2021.
\newblock ISSN 1573-1405.
\newblock \doi{10.1007/s11263-021-01453-z}.
\newblock URL \url{http://dx.doi.org/10.1007/s11263-021-01453-z}.

\bibitem[Hassija et~al.(2024)Hassija, Chamola, and et~al.]{hassija2024}
V.~Hassija, A.~Chamola, V.and~Mahapatra, and et~al.
\newblock Interpreting black-box models: A review on explainable artificial
  intelligence.
\newblock \emph{Cogn Comput}, 16:\penalty0 45--74, Jan. 2024.
\newblock \doi{10.1007/s12559-023-10179-8}.
\newblock URL \url{https://doi.org/10.1007/s12559-023-10179-8}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and
  Dean]{hinton2015distillingknowledgeneuralnetwork}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network, 2015.
\newblock URL \url{https://arxiv.org/abs/1503.02531}.

\bibitem[Huszár(2018)]{Husz_r_2018}
F.~Huszár.
\newblock Note on the quadratic penalties in elastic weight consolidation.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (11), Feb. 2018.
\newblock ISSN 1091-6490.
\newblock \doi{10.1073/pnas.1717042115}.
\newblock URL \url{http://dx.doi.org/10.1073/pnas.1717042115}.

\bibitem[(I.)()]{ibm2025}
I.~(I.).
\newblock Neural network.
\newblock URL \url{https://www.ibm.com/think/topics/neural-networks}.

\bibitem[Javed and
  White(2019)]{javed2019metalearningrepresentationscontinuallearning}
K.~Javed and M.~White.
\newblock Meta-learning representations for continual learning, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.12588}.

\bibitem[Jung et~al.(2021)Jung, Ahn, Cha, and
  Moon]{jung2021continuallearningnodeimportancebased}
S.~Jung, H.~Ahn, S.~Cha, and T.~Moon.
\newblock Continual learning with node-importance based adaptive group sparse
  regularization, 2021.
\newblock URL \url{https://arxiv.org/abs/2003.13726}.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitza, Veness,
  Guillaume~Desjardins, Milan, Quan, Ramalho, Agnieszk Grabska-Barwinska,
  Clopath, Kumaran, and Hadsell]{JK}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitza, J.~Veness, A.~A.~R.
  Guillaume~Desjardins, K.~Milan, J.~Quan, T.~Ramalho, D.~H. Agnieszk
  Grabska-Barwinska, C.~Clopath, D.~Kumaran, and R.~Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{arXiv:1612.00796v2}, 2017.

\bibitem[Li et~al.(2024)Li, Wu, and
  Braverman]{li2024fixeddesignanalysisregularizationbased}
H.~Li, J.~Wu, and V.~Braverman.
\newblock Fixed design analysis of regularization-based continual learning,
  2024.
\newblock URL \url{https://arxiv.org/abs/2303.10263}.

\bibitem[Li and Hoiem(2017)]{li2017learningforgetting}
Z.~Li and D.~Hoiem.
\newblock Learning without forgetting, 2017.
\newblock URL \url{https://arxiv.org/abs/1606.09282}.

\bibitem[Liu et~al.(2018)Liu, Masana, Herranz, de~Weijer, Lopez, and
  Bagdanov]{liu2018rotatenetworksbetterweight}
X.~Liu, M.~Masana, L.~Herranz, J.~V. de~Weijer, A.~M. Lopez, and A.~D.
  Bagdanov.
\newblock Rotate your networks: Better weight consolidation and less
  catastrophic forgetting, 2018.
\newblock URL \url{https://arxiv.org/abs/1802.02950}.

\bibitem[Loo et~al.(2020)Loo, Swaroop, and
  Turner]{loo2020generalizedvariationalcontinuallearning}
N.~Loo, S.~Swaroop, and R.~E. Turner.
\newblock Generalized variational continual learning, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.12328}.

\bibitem[Lopez-Paz and
  Ranzato(2022)]{lopezpaz2022gradientepisodicmemorycontinual}
D.~Lopez-Paz and M.~Ranzato.
\newblock Gradient episodic memory for continual learning, 2022.
\newblock URL \url{https://arxiv.org/abs/1706.08840}.

\bibitem[Ludkovski and Risk(2025)]{Ludkovski2025}
M.~Ludkovski and J.~Risk.
\newblock \emph{Gaussian Process Models for Quantitative Finance}.
\newblock Springer Nature Switzerland, Cham, 2025.
\newblock ISBN 978-3-031-80874-6.
\newblock \doi{10.1007/978-3-031-80874-6}.
\newblock URL \url{https://doi.org/10.1007/978-3-031-80874-6}.

\bibitem[Mallya et~al.(2018)Mallya, Davis, and
  Lazebnik]{mallya2018piggybackadaptingsinglenetwork}
A.~Mallya, D.~Davis, and S.~Lazebnik.
\newblock Piggyback: Adapting a single network to multiple tasks by learning to
  mask weights, 2018.
\newblock URL \url{https://arxiv.org/abs/1801.06519}.

\bibitem[Matt and Matt(2024)]{matt2024}
Matt and Matt.
\newblock How much data is generated per day, 12 2024.
\newblock URL
  \url{https://www.digitalsilk.com/digital-trends/how-much-data-is-generated-per-day/}.

\bibitem[Mcclelland et~al.(1995)Mcclelland, Mcnaughton, and
  O'Reilly]{Mcclelland1995}
J.~Mcclelland, B.~Mcnaughton, and R.~O'Reilly.
\newblock Why there are complementary learning systems in the hippocampus and
  neocortex: Insights from the successes and failures of connectionist models
  of learning and memory.
\newblock \emph{Psychological Review}, 102 3:\penalty0 419--457, 1995.
\newblock \doi{https://doi.org/10.1037/0033-295X.102.3.419}.

\bibitem[McCloskey and Cohen(1989)]{MCCLOSKEY1989109}
M.~McCloskey and N.~J. Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock volume~24 of \emph{Psychology of Learning and Motivation}, pages
  109--165. Academic Press, 1989.
\newblock \doi{https://doi.org/10.1016/S0079-7421(08)60536-8}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0079742108605368}.

\bibitem[Mirzadeh et~al.(2020)Mirzadeh, Farajtabar, Pascanu, and
  Ghasemzadeh]{mirzadeh2020understandingroletrainingregimes}
S.~I. Mirzadeh, M.~Farajtabar, R.~Pascanu, and H.~Ghasemzadeh.
\newblock Understanding the role of training regimes in continual learning,
  2020.
\newblock URL \url{https://arxiv.org/abs/2006.06958}.

\bibitem[Ratcliff(1990)]{Ratcliff1990ConnectionistMO}
R.~Ratcliff.
\newblock Connectionist models of recognition memory: constraints imposed by
  learning and forgetting functions.
\newblock \emph{Psychological RReview}, 97 2:\penalty0 285--308, 1990.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:18556305}.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and
  Lampert]{rebuffi2017icarlincrementalclassifierrepresentation}
S.-A. Rebuffi, A.~Kolesnikov, G.~Sperl, and C.~H. Lampert.
\newblock icarl: Incremental classifier and representation learning, 2017.
\newblock URL \url{https://arxiv.org/abs/1611.07725}.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{Shalev-Shwartz}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding Machine Learing: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Sudmann(2020)]{Sudmann2020}
A.~Sudmann.
\newblock \emph{K{\"u}nstliche neuronale Netzwerke als Black Box}, pages
  189--199.
\newblock Springer Fachmedien Wiesbaden, Wiesbaden, 2020.
\newblock ISBN 978-3-658-27852-6.
\newblock \doi{10.1007/978-3-658-27852-6_10}.
\newblock URL \url{https://doi.org/10.1007/978-3-658-27852-6_10}.

\bibitem[Titsias et~al.(2020)Titsias, Schwarz, de~G.~Matthews, Pascanu, and
  Teh]{titsias2020functionalregularisationcontinuallearning}
M.~K. Titsias, J.~Schwarz, A.~G. de~G.~Matthews, R.~Pascanu, and Y.~W. Teh.
\newblock Functional regularisation for continual learning with gaussian
  processes, 2020.
\newblock URL \url{https://arxiv.org/abs/1901.11356}.

\bibitem[van~de Ven et~al.(2022)van~de Ven, Tuytelaars, and
  Tolias]{vandeVen2022}
G.~M. van~de Ven, T.~Tuytelaars, and A.~S. Tolias.
\newblock Three types of incremental learning.
\newblock \emph{Nature Machine Intelligence}, 4\penalty0 (12):\penalty0
  1185--1197, Dec 2022.
\newblock ISSN 2522-5839.
\newblock \doi{10.1038/s42256-022-00568-3}.
\newblock URL \url{https://doi.org/10.1038/s42256-022-00568-3}.

\bibitem[Verwimp et~al.(2024)Verwimp, Aljundi, Ben-David, Bethge, Cossu,
  Gepperth, Hayes, Hüllermeier, Kanan, Kudithipudi, Lampert, Mundt, Pascanu,
  Popescu, Tolias, van~de Weijer, Liu, Lomonaco, Tuytelaars, and van~de
  Ven]{verwimp2024continuallearningapplicationsroad}
E.~Verwimp, R.~Aljundi, S.~Ben-David, M.~Bethge, A.~Cossu, A.~Gepperth, T.~L.
  Hayes, E.~Hüllermeier, C.~Kanan, D.~Kudithipudi, C.~H. Lampert, M.~Mundt,
  R.~Pascanu, A.~Popescu, A.~S. Tolias, J.~van~de Weijer, B.~Liu, V.~Lomonaco,
  T.~Tuytelaars, and G.~M. van~de Ven.
\newblock Continual learning: Applications and the road forward, 2024.
\newblock URL \url{https://arxiv.org/abs/2311.11908}.

\bibitem[Wang et~al.(2024)Wang, Zhang, Su, Zhu, Fellow, and IEEE]{LW}
L.~Wang, X.~Zhang, H.~Su, J.~Zhu, Fellow, and IEEE.
\newblock A comprehensive survey of continual learning: Theory and method and
  application, 2024.
\newblock URL \url{https://arxiv.org/abs/2302.00487}.

\bibitem[Wang et~al.(2022)Wang, Liu, Duan, and Tao]{Wang_Liu_Duan_Tao_2022}
Z.~Wang, L.~Liu, Y.~Duan, and D.~Tao.
\newblock Continual learning through retrieval and imagination.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  36\penalty0 (8):\penalty0 8594--8602, Jun. 2022.
\newblock \doi{10.1609/aaai.v36i8.20837}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/20837}.

\bibitem[Welling(2009)]{welling_2009}
M.~Welling.
\newblock Herding dynamical weights to learn.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, volume 382, page 141, 06 2009.
\newblock \doi{10.1145/1553374.1553517}.

\bibitem[Yin et~al.(2021)Yin, Farajtabar, Li, Levine, and
  Mott]{yin2021optimizationgeneralizationregularizationbasedcontinual}
D.~Yin, M.~Farajtabar, A.~Li, N.~Levine, and A.~Mott.
\newblock Optimization and generalization of regularization-based continual
  learning: a loss approximation viewpoint, 2021.
\newblock URL \url{https://arxiv.org/abs/2006.10974}.

\bibitem[Yoon et~al.(2018)Yoon, Yang, Lee, and
  Hwang]{yoon2018lifelonglearningdynamicallyexpandable}
J.~Yoon, E.~Yang, J.~Lee, and S.~J. Hwang.
\newblock Lifelong learning with dynamically expandable networks, 2018.
\newblock URL \url{https://arxiv.org/abs/1708.01547}.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and
  Ganguli]{zenke2017continuallearningsynapticintelligence}
F.~Zenke, B.~Poole, and S.~Ganguli.
\newblock Continual learning through synaptic intelligence, 2017.
\newblock URL \url{https://arxiv.org/abs/1703.04200}.

\bibitem[Zhao et~al.(2024)Zhao, Wang, Huang, and
  Lin]{zhao2024statisticaltheoryregularizationbasedcontinual}
X.~Zhao, H.~Wang, W.~Huang, and W.~Lin.
\newblock A statistical theory of regularization-based continual learning,
  2024.
\newblock URL \url{https://arxiv.org/abs/2406.06213}.

\end{thebibliography}
