%%%%%%%%%%%%%%%%%%% Proof that CRR is unbiased %%%%%%%%%%%%%%%%%%%
\begin{proof}
Let $D_1 = (X^{(1)}, y^{(1)})$ and $D_2 = (X^{(2)}, y^{(2)})$ be two conditionally independent linear regression problems, with $y^{(i)} \sim N(X^{(i)} w^*, \sigma^2 I), i \in \{1,2\}$.
Since $\hat{w}^{(1)}$ is estimated OLS, we already know that it is unbiased so we directly continue with derivation of $\hat{w}^{(2)}$:

\begin{equation}
	\begin{split}
		\hat{w}^{(2)} &= \arg\min_{w} \frac{1}{n}\lVert y^{(2)} - X^{(2)\top} w\rVert_2^2 + \pen(w)  \Leftrightarrow \\
		0 &= \nabla [\frac{1}{n}\lVert y^{(2)} - X^{(2)\top} w\rVert_2^2 + \pen(w)] \\
		&= \nabla [\frac{1}{n} (y^{(2)} - X^{(2)}w)^\top (y^{(2)} - X^{(2)}w) + \lambda(w - \hat{w}^{(1)})^\top (w - \hat{w}^{(1)})] \\
		&= -\frac{2}{n} X^{(2)\top} y^{(2)} + \frac{2}{n} wX^{(2)\top} X^{(2)} + 2\lambda w - 2\lambda \hat{w}^{(1)} \Rightarrow \\
		\hat{w}^{(2)} &= (X^{(2)\top} X^{(2)}+\lambda n I)^{-1}(X^{(2)\top} y^{(2)} +\lambda n \hat{w}^{(1)})
	\end{split}
\end{equation}
Define $A := (X^{(2)\top} X^{(2)}+\lambda n I)^{-1}$, then $\mathbb{E}[\hat{w}^{(2)}]$ is
\begin{equation}
	\begin{split}
		\mathbb{E}[\hat{w}^{(2)}] &= \mathbb{E}[(X^{(2)\top} X^{(2)}+\lambda n I)^{-1}(X^{(2)\top} y_2 +\lambda n \hat{w}^{(1)})] \\
		&= A \left( \mathbb{E}[X^{(2)\top}(X^{(2)} w^{(2)} + \varepsilon^{(2)})]  + \lambda n w^* \right) \\
		&= A (X^{(2)\top} X^{(2)}w^* + \lambda n w^* ) \\
		&= A A^{-1} w^* \\
		&= w^*
	\end{split}
\end{equation}
\end{proof}