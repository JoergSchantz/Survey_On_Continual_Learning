%%%%%%%%%%%%%%%%%%% Proof that CRR is biased %%%%%%%%%%%%%%%%%%%
Let $D_1 = (X_1, y_1)$ and $D_2 = (X_2, y_2)$ be two independent linear regression problems, with $y_i \sim N(X_i w_i, \sigma_i^2 I), i \in \{1,2\}$.
Begin with matrix form of $\hat{w_2}$:
\begin{equation}
	\begin{split}
		\hat{w_2} &= \arg\min_{w_2} \frac{1}{n}\lVert y_2 - X_2^\top w_2\rVert_2^2 + \pen(w_2)  \Leftrightarrow \\
		0 &= \nabla [\frac{1}{n}\lVert y_2 - X_2^\top w_2\rVert_2^2 + \pen(w_2)] \\
		&= \nabla [\frac{1}{n} (y_2 - X_2w_2)^\top (y_2 - X_2w_2) + \lambda(w_2 - \hat{w_1})^\top (w_2 - \hat{w_1})] \\
		&= -\frac{2}{n} X_2^\top Y_2 + \frac{2}{n} w_2X_2^\top X_2 + 2\lambda w_2 - 2\lambda \hat{w_1} \\
		\hat{w_2} &= (X_2^\top X_2+\lambda n I)^{-1}(X_2^\top y_2 +\lambda n \hat{w_1})
	\end{split}
\end{equation}
Continue with $\mathbb{E}[\hat{w_2}]$. Define $A := (X_2^\top X_2+\lambda n I)^{-1}$.
\begin{equation}
	\begin{split}
		\mathbb{E}[\hat{w_2}] &= \mathbb{E}[(X_"^\top X_2+\lambda n I)^{-1}(X_2^\top y_2 +\lambda n \hat{w_1})] \\
		&= A ( \lambda n \hat{w_1} + \mathbb{E}[X_2^\top(X_2 w_2 + e_2)] ) \\
		&= A \lambda n \hat{w_1} + A X_2^\top X_2 \mathbb{E}[w_2] \\
		&\neq \mathbb{E}[w_2]
	\end{split}
\end{equation}
For $\lambda  = 0 \Rightarrow A = (X_2^\top X_2)^{-1} \Rightarrow \mathbb{E}(\hat{w_2}) = \mathbb{E}(w_2)$.\\


End with $\mathbb{V}(\hat{w_2})$
\begin{equation}
	\begin{split}
		\mathbb{V}(\hat{w_2}) &= \mathbb{V}(A (X_2^\top y_2 + \lambda n \hat{w_1})) \\
		&= \mathbb{V}(y_2) A^\top X_2^\top X_2 A\\
		&= \sigma_2^2I A^\top X_2^\top X_2 A
	\end{split}
\end{equation}.