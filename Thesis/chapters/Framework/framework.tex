%%%%%%%%%%%%%%%%%%% framework %%%%%%%%%%%%%%%%%%%
Throughout literature continual learning in a statistical sense means modeling a joint probability distribution $\mathbb{P}^{(T)}$, which is allowed to expand indefinitely \cite{LW}. $T$ samples $D_t, t \in {1, ...,T}$ from different distributions $\mathbb{P}_t$ are processed sequentially. A single sample has the form $D_t = (x^{(t)}_i,y^{(t)}_i)$ with $x^{(t)}_i$ being the $i$-th covariate and $y^{(t)}_i$ the dependent variable. The $D_t$ are assumed to be conditionally independent but not necessarily identically distributed \cite{LW}. Each tuple $(D_t, \mathbb{P}_t)$ may correspond to a distinct regression or classification task that is to be learned. The goal is to train a single model which is able to perform well on all tasks, although it is trained sequentially and cannot necessarily revisit prior tasks. 