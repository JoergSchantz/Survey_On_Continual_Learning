%%%%%%%%%%%%%%%%%%% Note on evaluation %%%%%%%%%%%%%%%%%%%
At last it is important to note that this thesis does not provide information about the compared performance of each of the presented regularization techniques. Despite the fact that all authors have conducted their own experiments, a formal comparison, through survey work alone, is not possible at the moment. Each research team has used different data sets and the overlap of used reference models is very small. Additionally, ordinary CL, CRR and GR serve a strictly theoretical purpose, since their assumptions strongly restrict their real world applications. The teams behind them have only provided experimental proof of concepts via simulated data and acknowledge their limitations.

Nevertheless, the conducted experiments highlight the influence of EWC on the field. It is used as a reference model for the majority of the presented regularization methods \cite{zenke2017continuallearningsynapticintelligence, aljundi2018memoryawaresynapseslearning, yoon2018lifelonglearningdynamicallyexpandable, jung2021continuallearningnodeimportancebased, titsias2020functionalregularisationcontinuallearning}.

Another point worth highlighting is that MINST and CIFAR are the most used data bases for the showcased methodologies \cite{zenke2017continuallearningsynapticintelligence,yoon2018lifelonglearningdynamicallyexpandable, jung2021continuallearningnodeimportancebased, Wang_Liu_Duan_Tao_2022}. This raises the question if their overuse biases research towards a race for the "best fit".