Consider a general continual learning (GCL) problem. $T$ individual tasks $t \in \{1, ..., T\}$ arrive in sequence. Each task has a sample set $D_t$ where every $D_t$ may be drawn from its own population. Hence they are assumed to be independent but not identically distributed. A single sample has the form $D_t = (x^{(t)}_i,y^{(t)}_i)$ with $x^{(t)}_i$ being the $i$-th feature vector and $y^{(t)}_i$ the corresponding target.\\
In regard to the distribution of $Y = \{Y^{(1)}, ..., Y^{(T)}\}$ one can differentiate between three different types of CL:\\
\textit{Domain-incremental} learning considers only one big task where its data from one population arrives in multiple batches. Thus $\{Y^{(t)}\}=\{Y^{(t+1)}\}$ and $D_t \sim P(Y^{(t)})$ iid.\\
\textit{Class-incremental} learning describes the problem of learning 
\textit{Task-incremental} learning
